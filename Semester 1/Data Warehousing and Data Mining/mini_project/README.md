# <center> Data Warehousing and Data Mining Mini Project</center>



## <center> Hierarchical Clustering and Classification Tree on 'Iris' dataset</center>

----

#### Implementation details:

1. ***Build platform*** : [Orange3 v3.30.2](https://orangedatamining.com/download/#windows) on Windows 10 Home Edition.
2. ***Target platform*** : Any operating system capable of running [Anaconda](https://www.anaconda.com/), [Miniconda](https://docs.conda.io/en/latest/miniconda.html), or [Python3](https://www.python.org/). (ordered from most favorable to least favorable)
3. ***File format*** : Orange Workflow (.ows file)
4. ***Additional packages used*** : None



#### File description

1. ***File name***: [mini_project.ows](mini_project.ows)

2. ***Dataset used***: Iris.tab (Preinstalled)

3. ***Output Screenshots subdirectory***:[screenshots](./screenshots/)



## <center> Concepts </center>

### <center> Hierarchical Clustering</center>

> In [data mining](https://en.wikipedia.org/wiki/Data_mining) and [statistics](https://en.wikipedia.org/wiki/Statistics), **hierarchical clustering** (also called **hierarchical cluster analysis** or **HCA**) is a method of [cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis) which seeks to build a [hierarchy](https://en.wikipedia.org/wiki/Hierarchy) of clusters. Strategies for hierarchical clustering generally fall into two types:
>
> - **Agglomerative**: This is a "[bottom-up](https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design)" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
> - **Divisive**: This is a "[top-down](https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design)" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.
>
> In general, the merges and splits are determined in a [greedy](https://en.wikipedia.org/wiki/Greedy_algorithm) manner. The results of hierarchical clustering are usually presented in a [dendrogram](https://en.wikipedia.org/wiki/Dendrogram). \- [Wikipedia](https://en.wikipedia.org/wiki/Hierarchical_clustering)



### <center> Classification Tree</center>

> A **decision tree**  is a [decision support](https://en.wikipedia.org/wiki/Decision_support_system) tool that uses a [tree-like](https://en.wikipedia.org/wiki/Tree_(graph_theory)) [model](https://en.wikipedia.org/wiki/Causal_model) of decisions and their possible consequences, including [chance](https://en.wikipedia.org/wiki/Probability) event outcomes, resource costs, and [utility](https://en.wikipedia.org/wiki/Utility). It is one way to display an [algorithm](https://en.wikipedia.org/wiki/Algorithm) that only contains conditional control statements.
>
> Decision trees are commonly used in [operations research](https://en.wikipedia.org/wiki/Operations_research), specifically in [decision analysis](https://en.wikipedia.org/wiki/Decision_analysis), to help identify a strategy most likely to reach a [goal](https://en.wikipedia.org/wiki/Goal), but are also a popular tool in [machine learning](https://en.wikipedia.org/wiki/Decision_tree_learning).
>
> A decision tree is a [flowchart](https://en.wikipedia.org/wiki/Flowchart)-like structure in which each internal node represents a "test" on an  attribute (e.g. whether a coin flip comes up heads or tails), each  branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The  paths from root to leaf represent classification rules.
>
> In [decision analysis](https://en.wikipedia.org/wiki/Decision_analysis), a decision tree and the closely related [influence diagram](https://en.wikipedia.org/wiki/Influence_diagram) are used as a visual and analytical decision support tool, where the [expected values](https://en.wikipedia.org/wiki/Expected_value) (or [expected utility](https://en.wikipedia.org/wiki/Expected_utility)) of competing alternatives are calculated. \- [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree)
>
> 